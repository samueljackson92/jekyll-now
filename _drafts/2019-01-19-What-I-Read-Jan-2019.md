---
layout: post
title: Reading Round-up; January 2019
---

A monthly round-up of what I've been reading this month.

### Papers
- [Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366). What if neural networks didn't have layers? It's a intriguing question from one of the papers presented at NeurIPS this year. The authors show that they can train a RNN using differential equations instead of layers.

- [Manipulating and Measuring Model Interpretability](https://arxiv.org/abs/1802.07810)

- [Machine learning modeling of superconducting critical temperature](https://arxiv.org/abs/1709.02727). I chose this paper just before Christmas to present at my local ML journal club. The premise of the paper is quite simple. Can we classify superconductors using a neural network by their critical temperature using input features derived from properties about the material.

### Blog Posts

- [AlphaFold: Using AI for scientific discovery](https://deepmind.com/blog/alphafold/). An interesting new project from the DeepMind team. They use a neural network trained on the genetic sequence of proteins to predict the amino acid distance & bond angles. Then used a generative network to predict new structural motifs to improve the score of a proposed structure. There's a paper pending with all the gory details that I'll eagerly await. Presumably this would would also be almost immediately applicable to RNA folding as well.

- [How to Use t-SNE Effectively](https://distill.pub/2016/misread-tsne/). An older post from the great [distill](http://distill.pub) site. This is a beautiful guide on how to use read (and more importantly not to misread) plots produced by the t-SNE algorithm for dimensionality reduction. In particular it focuses on how abusing the perplexity parameter can produce misleading structure within plots that can be easily misinterpreted. t-SNE has a sort of "magnification" effect where points in the center of a plot appear closer and points further out relatively further away. The key headline takeaways were:
     1. Hyper-parameters (particularly perplexity and learning rate) make a big impact on producing an interpretable plot.
     2. Cluster sizes mean nothing
     3. Distance between clusters is not reliable.
     4. Random noise doesn't always look random
     5. Shapes can be visible, but can be distorted by t-SNE's magnification effect.
     6. For topological analysis it's worth looking at more than one plot (with varied perplexity).
