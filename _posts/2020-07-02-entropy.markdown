---
layout: post
title: Notes on Entropy 
---

Information entropy is defined as the average level of "uncertainty" inherent in a random variables possible outcomes. From the book [Deep Learning](https://www.deeplearningbook.org/):

> The basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred.

> … the Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution. It gives a lower bound on the number of bits […] needed on average to encode symbols drawn from a distribution P.

 - A **low probability** event is very informative. 
 - A **high probability** event is not very informative. 
 
Entropy is highest where in random variables that have a lot of variance. For a random variable $X$ with outcomes $x_i$ each with probability $P_X(x_i)$ entropy ($H(X)$) is defined as:

$$
H(X) = - \sum_i P_X(x_i)\ log\ P_X(x_i)
$$

This can alternatively be written as:

$$
H(X) = \sum_i P_X(x_i)I_X(x_i)
$$

Where $I_X$ is the self information of $X$.

Information entropy is measured in units of bit if the base 2 logarithm is used. If the natural log is used then it is in units of nats



### Calculating Self-Information for a single event

Suppose we have a single trial of a Bernoulli discrete random variable. The classic example is the coin flip. For different coin biases we can compute the self-information ($I_X$) of the event occurring. Looking at the graph below shows that the number of bits required to encode the information about the event increases as it approaches 0. The chance of a head occuring is low, then the event is very rare, and therefore contains requires more information (or bits) to describe it. 


```python
p_x = np.linspace(0, 1, 30)
information = -np.log2(p_x)

plt.plot(p_x, information)

plt.ylabel('Information (Bits)')
plt.xlabel('$P_{Heads}(x_i)$')
```

![png](/assets/images/2020-07-02/entropy_4_2.png)


### Calculating the Information Entropy for a Random Variable

For a Bernoulli random variable, such as a coin flip we can calculate the entropy using the formula $H(X)$ given above. In the case of Bernoulli which can have two possible events (head or tails) the parameter $\theta$ defines the probability of both the head an tail events, because $P_{Heads}(\theta) = (1-P_{Tails}(\theta))$. Therefore the entropy equation for a Bernoulli random variable is:

$$
H(X) = - [P_{Heads}(\theta)\ log\ P_{Heads}(\theta) + P_{Tails}(\theta)\ log\ P_{Tails}(\theta)]
$$
$$
H(X) = - [P_{Heads}(\theta)\ log\ P_{Heads}(\theta) + (1-P_{Heads}(\theta))\ log\ (1-P_{Heads}(\theta))]
$$

We if we plot $H(x)$ as a function of $\theta$ we can see that entropy is highest when we have a completely fair dice ($\theta = 0.5$). This is intuitive: we need more bits to correctly encode a information when the "amount of randomness" is high. Conversely, at either extreme, when $\theta = 1$ or $\theta = 0$ then we effectively need 0 bits to encode all the information correctly. It's a complete certainty either way!


```python
p_x = np.linspace(0, 1, 100)
info_heads = -p_x*np.log2(p_x)
info_tails = -(1-p_x)*np.log2(1-p_x)
entropy = (info_heads + info_tails)

plt.plot(p_x, entropy)

plt.ylabel('$H(x)$ (bits)')
plt.xlabel('$\\theta$')
```


![png](/assets/images/2020-07-02/entropy_6_2.png)


Let's play with a more complicated example can be given with an example using a Poisson distribution. The Poisson distribution takes one parameter $\lambda$ which is both the expected value of the distribution and its variance. For example, a Poisson distribution with $\lambda = 5$ looks like the following. Here we're only plotting the probability of counts between the range 0-15.


```python
from scipy import stats
p = stats.poisson(5)

x = np.arange(0, 15)
prob = p.pmf(x)
plt.bar(x, prob)
plt.xlabel('$x$')
plt.ylabel('$P(x)$')
```

![png](/assets/images/2020-07-02/entropy_8_1.png)


For each possible event we can calculate the self-information as follows by taking the $log$ of the probability to the event.


```python
plt.bar(x, -np.log2(prob))
plt.ylabel('Self-Information')
plt.xlabel('Bin')
```




    Text(0.5, 0, 'Bin')




![png](/assets/images/2020-07-02/entropy_10_1.png)


We can visualise how the probability mass function changes for different values of $\lambda$.


```python
probs = np.array([stats.poisson(i).pmf(x) for i in np.linspace(1, 15, 30)]).T
plt.matshow(probs)
plt.xlabel('$\lambda$')
plt.ylabel('$P(x)$')
```

![png](/assets/images/2020-07-02/entropy_12_1.png)


Likewise, we can visualise how the self-information changes for different values of $\lambda$. We can see that the self information is always lowest wherever the PMF is highest.


```python
plt.matshow(-np.log2(probs))
plt.xlabel('$\lambda$')
plt.ylabel('Self-Information')
```




    Text(0, 0.5, 'Self-Information')




![png](/assets/images/2020-07-02/entropy_14_1.png)


Calculating the entropy of the Poisson distribution involves taking an infinite sum of all possible events (even those which are very unlikely). For simplicty, `scipy.stats.poisson` includes an entropy function to calculate this for us. We can see that as $\lambda$ increases, so does the entropy, but entropy also starts to plateau  with higher values of $\lambda$.


```python
xs = np.linspace(1, 100, 100)
entropy = np.array([stats.poisson(i).entropy() for i in xs]).T
plt.plot(xs, entropy)
plt.xlabel('$\lambda$')
plt.ylabel('Entropy (bits)')
```

![png](/assets/images/2020-07-02/entropy_16_1.png)


Sources:
 - [Entropy (Information Theory) Wikipedia](https://en.wikipedia.org/wiki/Entropy_(information_theory))
 - [A Gentle Introduction to Information Entropy](https://machinelearningmastery.com/what-is-information-entropy/)
